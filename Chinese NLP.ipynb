{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文语料库的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding='utf-8'\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba # 结巴分词\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim加载预训练中文分词embedding\n",
    "cn_model = KeyedVectors.load_word2vec_format('chinese_word_vectors/sgns.zhihu.bigram', \n",
    "                                          binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 什么是embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "地图就是对于现实地理的embedding，现实的地理地形的信息其实远远超过三维 但是地图通过颜色和等高线等来最大化表现现实的地理信息。\n",
    "\n",
    "词的embedding也就是用固定的维度来最大化表现词的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作用是把松散的one-hot字典表示法变为固定大小的向量，能方便的通过向量cos求得词语间是否近似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量的长度为300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.839300e-02, -4.797640e-01, -1.115136e+00, -2.417000e-02,\n",
       "        2.293440e-01, -8.419710e-01, -7.340420e-01,  1.342570e-01,\n",
       "       -3.668080e-01, -7.464200e-02, -5.322120e-01,  2.076460e-01,\n",
       "       -8.524420e-01, -7.274490e-01,  4.140000e-02, -1.320350e-01,\n",
       "        3.192460e-01, -1.300115e+00, -2.239420e-01, -1.488940e-01,\n",
       "       -2.176700e-02,  5.542770e-01, -1.617870e-01,  4.242610e-01,\n",
       "        9.656400e-02, -8.644500e-02, -6.494450e-01, -2.400440e-01,\n",
       "       -1.060745e+00, -3.220000e-02, -6.661110e-01,  4.554000e-03,\n",
       "        8.424700e-02,  1.615070e-01,  1.908820e-01, -7.867680e-01,\n",
       "       -7.221560e-01,  4.915430e-01, -3.853340e-01,  1.394420e-01,\n",
       "       -2.067480e-01, -1.179910e-01,  4.989760e-01,  1.117296e+00,\n",
       "        3.776300e-02, -5.642530e-01,  2.914660e-01, -1.017945e+00,\n",
       "       -4.110000e-04, -1.349609e+00, -1.054234e+00, -5.482800e-01,\n",
       "       -5.235040e-01, -1.695090e-01, -3.141680e-01,  2.332400e-02,\n",
       "       -1.379530e-01,  1.586490e-01, -1.511060e-01,  7.714800e-02,\n",
       "       -1.397368e+00,  7.747000e-01,  6.073500e-02,  1.285500e-01,\n",
       "        1.454050e-01,  9.217590e-01,  1.271566e+00, -3.485310e-01,\n",
       "       -2.336920e-01,  6.258770e-01,  6.768330e-01,  1.897570e-01,\n",
       "       -6.853400e-01, -8.943340e-01, -1.027790e-01,  5.198200e-02,\n",
       "       -2.963030e-01, -1.751300e-01,  8.821460e-01,  2.370690e-01,\n",
       "        6.718860e-01, -2.149690e-01,  1.020398e+00,  5.198020e-01,\n",
       "       -2.290590e-01, -1.992540e-01,  8.058980e-01,  1.080800e-01,\n",
       "       -5.367980e-01,  4.581030e-01,  1.160830e-01, -7.562600e-02,\n",
       "       -2.180640e-01, -3.061200e-02, -3.286900e-02,  5.522890e-01,\n",
       "        2.352710e-01, -5.003920e-01,  7.026770e-01, -1.616090e-01,\n",
       "       -2.005570e-01,  3.756000e-01,  5.245640e-01, -1.574730e-01,\n",
       "       -7.953680e-01, -5.086190e-01,  2.331840e-01, -2.865150e-01,\n",
       "        1.782690e-01,  1.368600e-02,  2.379940e-01, -7.601100e-02,\n",
       "       -2.729680e-01,  4.376320e-01, -2.988060e-01,  4.156870e-01,\n",
       "       -4.945520e-01,  3.593330e-01, -1.009115e+00, -4.803480e-01,\n",
       "        6.054750e-01,  8.479300e-02,  4.621920e-01, -4.629030e-01,\n",
       "        7.427400e-01, -7.965960e-01, -6.563420e-01, -8.866620e-01,\n",
       "        1.065670e+00, -8.012910e-01,  7.462440e-01, -2.394000e-03,\n",
       "       -3.920700e-01, -3.130000e-02, -1.515190e-01, -5.091870e-01,\n",
       "       -2.953260e-01, -1.255618e+00,  8.087890e-01, -2.635730e-01,\n",
       "       -3.070410e-01, -5.169340e-01,  4.987470e-01, -5.419130e-01,\n",
       "        1.627500e-02, -9.557680e-01, -1.644800e-02,  5.001830e-01,\n",
       "        6.465400e-01,  5.181200e-01,  2.701270e-01,  4.317800e-02,\n",
       "       -3.279320e-01,  9.538700e-02,  2.937160e-01,  6.846600e-01,\n",
       "        2.279860e-01, -4.293020e-01, -3.213040e-01,  5.362880e-01,\n",
       "        2.003200e-02,  2.415560e-01, -4.666750e-01, -8.724290e-01,\n",
       "       -3.585200e-02, -8.919300e-02, -5.540730e-01, -6.082400e-02,\n",
       "       -1.174260e-01,  1.193937e+00,  4.195000e-03, -8.461400e-02,\n",
       "       -1.148960e-01,  8.076230e-01,  2.672300e-01,  4.617250e-01,\n",
       "       -3.022110e-01,  2.848190e-01, -3.396240e-01, -4.205710e-01,\n",
       "       -4.130840e-01,  2.743450e-01,  1.511720e-01,  6.141200e-02,\n",
       "       -3.098000e-02,  1.484540e-01, -9.363000e-02,  3.210100e-01,\n",
       "       -1.844870e-01,  3.906650e-01, -1.089790e+00, -3.555680e-01,\n",
       "        6.126640e-01, -4.393100e-01,  3.623290e-01,  1.781030e+00,\n",
       "        5.195970e-01,  8.505830e-01, -3.569720e-01, -4.434850e-01,\n",
       "        7.953400e-02,  1.140449e+00, -9.874770e-01, -2.173600e-01,\n",
       "        7.340860e-01, -1.108746e+00, -1.152000e-03,  8.025210e-01,\n",
       "        6.061580e-01, -4.461870e-01, -8.219200e-02,  5.945800e-01,\n",
       "       -9.713920e-01,  4.506220e-01,  8.531890e-01, -4.250800e-02,\n",
       "        1.041038e+00,  3.161630e-01, -2.645170e-01, -5.149040e-01,\n",
       "        2.564590e-01,  6.413540e-01,  7.888280e-01, -1.063660e-01,\n",
       "       -4.220340e-01,  6.287000e-02, -3.895100e-02, -6.033820e-01,\n",
       "       -2.395370e-01,  1.250760e-01, -5.053910e-01,  2.793400e-02,\n",
       "        7.517950e-01,  3.187620e-01,  2.297700e-02, -1.031147e+00,\n",
       "       -1.828930e-01,  2.706070e-01,  8.850200e-02,  5.887410e-01,\n",
       "       -1.492820e-01,  2.559340e-01,  4.516760e-01,  1.399390e-01,\n",
       "        3.189000e-01, -8.326590e-01, -5.387700e-01, -1.749700e-01,\n",
       "        5.551800e-01,  5.045870e-01,  1.288650e+00, -5.908150e-01,\n",
       "        5.999590e-01,  2.635050e-01,  3.149260e-01, -5.479960e-01,\n",
       "       -8.340520e-01,  1.822230e-01, -3.212430e-01, -2.548100e-02,\n",
       "       -6.457060e-01,  1.006940e+00, -5.028950e-01,  3.479760e-01,\n",
       "        1.179240e-01, -2.093370e-01, -5.488030e-01, -3.894220e-01,\n",
       "       -3.947710e-01, -1.174873e+00,  3.125200e-01, -3.532170e-01,\n",
       "        1.002431e+00,  3.502610e-01,  5.336100e-02,  5.594900e-02,\n",
       "        1.776030e-01,  2.380300e-02,  1.080670e-01,  3.838230e-01,\n",
       "       -2.427590e-01, -3.507170e-01, -5.680060e-01,  1.082881e+00,\n",
       "       -3.959800e-01, -3.701620e-01,  1.222100e-01,  5.378100e-02,\n",
       "        5.046450e-01, -4.681460e-01, -9.621410e-01,  1.181160e-01,\n",
       "       -6.015840e-01, -9.915000e-02, -4.528940e-01, -2.345000e-01,\n",
       "        3.386040e-01, -1.556280e-01,  2.504670e-01,  1.514210e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每一个词都对应一个长度为300的向量（300维）\n",
    "embedding_dim = cn_model['你好'].shape[0]\n",
    "print('词向量的长度为{}'.format(embedding_dim))\n",
    "cn_model['你好']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66128117"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 相似度其实就是：dot（'橘子'/|'橘子'|， '橙子'/|'橙子'| ）\n",
    "np.dot(cn_model['橘子']/np.linalg.norm(cn_model['橘子']), \n",
    "cn_model['橙子']/np.linalg.norm(cn_model['橙子']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66128117"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算相似度\n",
    "cn_model.similarity('橘子', '橙子')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.vocab['老师'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'老师'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.index2word[215]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('并不知道', 0.5925602912902832),\n",
       " ('不晓得', 0.5772397518157959),\n",
       " ('明白', 0.5554894804954529),\n",
       " ('不知', 0.5548974275588989),\n",
       " ('晓得', 0.5365551114082336),\n",
       " ('真不知道', 0.5153868794441223),\n",
       " ('知晓', 0.4782266914844513),\n",
       " ('天知道', 0.46890372037887573),\n",
       " ('了解到', 0.455231636762619),\n",
       " ('深知', 0.4438132047653198)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出最相近的词，余弦相似度\n",
    "cn_model.most_similar(positive=['知道'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 老师 会计师 程序员 律师 医生 老人 中:\n",
      "不是同一类别的词为: 老人\n"
     ]
    }
   ],
   "source": [
    "# 找出不同的词\n",
    "test_words = '老师 会计师 程序员 律师 医生 老人'\n",
    "test_words_result = cn_model.doesnt_match(test_words.split())\n",
    "print('在 '+test_words+' 中:\\n不是同一类别的词为: %s' %test_words_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('劈腿', 0.5849199295043945)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.most_similar(positive=['女人','出轨'], negative=['男人'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 酒店评价分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练语料:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "使用谭松波老师的酒店评论语料\n",
    "训练样本分别被放置在两个文件夹里： 分别的pos和neg，每个文件夹里有2000个txt文件，每个文件内有一段评语，共有4000个训练样本，这样大小的样本数据在NLP中属于非常迷你的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得样本的索引，样本存放于两个文件夹中，\n",
    "# 分别为 正面评价'pos'文件夹 和 负面评价'neg'文件夹\n",
    "# 每个文件夹中有2000个txt文件，每个文件中是一例评价\n",
    "import os\n",
    "pos_txts = os.listdir('pos')\n",
    "neg_txts = os.listdir('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正面评价共: 1942\n",
      "负面评价共：1911\n"
     ]
    }
   ],
   "source": [
    "print( '正面评价共: '+ str(len(pos_txts)) )\n",
    "print('负面评价共：' + str(len(neg_txts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我下到的语料库是乱码的，需要解决这个问题 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'GB2312', 'confidence': 0.99, 'language': 'Chinese'}\n"
     ]
    }
   ],
   "source": [
    "import chardet \n",
    "f = open('pos.5.txt','rb')\n",
    "result = chardet.detect(f.read())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最终探明我的语料库是用GB2312编码的（不是ANSI），接下来把它转为utf-8格式则恢复正常 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终没能实现notepad++批量转码，插件无法安装，这里还是需要探索。另外自己写脚本批量转码也没有成功，但下面这段批量转码可以正常操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI文件转UTF-8\n",
    "import codecs\n",
    "import os\n",
    " \n",
    "# 文件所在目录\n",
    "file_path = \"C:\\\\Users\\\\CaiTi\\\\neg\"\n",
    "files = os.listdir(file_path)\n",
    "count = 0\n",
    " \n",
    "for file in files:\n",
    "    file_name = file_path + '\\\\' + file\n",
    "    f = codecs.open(file_name, 'r', 'GB2312')\n",
    "    try:\n",
    "        ff = f.read()\n",
    "        file_object = codecs.open(file_path + '\\\\' + file, 'w', 'utf-8')\n",
    "        file_object.write(ff)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。\n",
      "\n",
      "房间本身很好。\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('pos/pos.10.txt',encoding='utf-8')\n",
    "ff = f.read()\n",
    "print(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们将所有评价内容放入一个list里面\n",
    "\n",
    "# 存储所有评价，每例评价为一条string\n",
    "train_texts_orig = []\n",
    "\n",
    "for i in range(len(pos_txts)):\n",
    "    # 对2000个正评价循环，打开第i个文件\n",
    "    with open('pos/'+pos_txts[i], encoding='utf-8', errors = 'ignore') as f:\n",
    "        text = f.read().strip()\n",
    "        # 在list中添加这条评价的string\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()\n",
    "for i in range(len(neg_txts)):\n",
    "    with open('neg/'+neg_txts[i],encoding='utf-8',errors='ignore') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3853"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此我们获得了***3853***条评价的list（4000条中有一部分在转码中损失掉了），接下来我们要导入所需类库，并做预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词和tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "首先我们去掉每个样本的标点符号，然后用jieba分词，jieba分词返回一个生成器，没法直接进行tokenize，所以我们将分词结果转换成一个list，并将它索引化，这样每一例评价的文本变成一段索引数字，对应着预训练词向量模型中的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行分词和tokenize\n",
    "\n",
    "# train_tokens 是一个长长的list, 其中包含4000个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "\n",
    "# 对上一步得到的4000句评价循环\n",
    "for text in train_texts_orig:\n",
    "    #去掉标点,(把text中的中文标点替换成空字符串)\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    #结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list(现在cut_list是一句话里每一个词语组成的list)\n",
    "    cut_list = [i for i in cut]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引长度标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为每段评语的长度是不一样的，我们如果单纯取最长的一个评语，并把其他评填充成同样的长度，这样十分浪费计算资源，所以我们取一个折衷的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23 86 16 ... 44 45  0]\n"
     ]
    }
   ],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.74305735790293"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均tokens的长度\n",
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最长的评价tokens的长度\n",
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 评价长度的分布\n",
    "plt.hist(num_tokens, bins = 100)\n",
    "plt.xlim((0,1000))\n",
    "plt.ylabel('number of tokens')\n",
    "plt.xlabel('length of tokens')\n",
    "plt.title('Distribution of tokens length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "# (取完log后会符合正态分布)， 但意义是什么\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9563976122501947"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens的长度为843时，大约95%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义一个function，用来把索引转换成可阅读的文本，这对于debug很重要。（reverse后会失去标点）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来将tokens转换为文本\n",
    "\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'早餐太差无论去多少人那边也不加食品的酒店应该重视一下这个问题了房间本身很好'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse = reverse_tokens(train_tokens[0])\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。\\n\\n房间本身很好。'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始文本\n",
    "train_texts_orig[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意之后解决中文乱码问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们来为模型准备embedding matrix（词向量矩阵），根据keras的要求，我们需要准备一个维度为$(numwords, embeddingdim)$的矩阵，num words代表我们使用的词汇的数量，emdedding dimension在我们现在使用的预训练词向量模型中是300，每一个词汇都用一个长度为300的向量表示。\n",
    "注意我们只选择使用前50k个使用频率最高的词，在这个预训练词向量模型中，一共有260万词汇量，如果全部使用在分类问题上会很浪费计算资源，因为我们的训练样本很小，一共只有4k，如果我们有100k，200k甚至更多的训练样本时，在分类问题上可以考虑减少使用的词汇量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只使用前50000个词\n",
    "num_words = 50000\n",
    "# 初始化embedding_matrix，填充全为0的50000*300的矩阵\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# 从之前预训练的cn模型中循环频率最高的前50000个词，把它对应的词向量填充到我们的embedding矩阵里\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "# 类型要定为float32\n",
    "embedding_matrix = embedding_matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取其中一个index检查index是否对应\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum(cn_model[cn_model.index2word[333]] == embedding_matrix[333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最后在确认一下维度\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding（填充）和truncating（修剪）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把文本转换为tokens（索引）之后，每一串索引的长度并不相等，所以为了方便模型的训练我们需要把索引的长度标准化，上面我们选择了843这个可以涵盖95%训练样本的长度，接下来我们进行padding和truncating，我们一般采用'pre'的方法，这会在文本索引的前面填充0，因为根据一些研究资料中的实践，如果在文本索引后面填充0的话，会对模型造成一些不良影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行padding 和 truncating，输入的train_tokens 是一个list\n",
    "# 返回的train_pad 是一个numpy array\n",
    "train_pad = pad_sequences(train_tokens, maxlen = max_tokens, padding = 'pre', truncating = 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超过五万个词向量的词用0代替\n",
    "train_pad[train_pad>=num_words] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,   290,  3053,    57,   169,    73,     1,    25,\n",
       "       11216,    49,   163, 15985,     0,     0,    30,     8,     0,\n",
       "           1,   228,   223,    40,    35,   653,     0,     5,  1642,\n",
       "          29, 11216,  2751,   500,    98,    30,  3159,  2225,  2146,\n",
       "         371,  6285,   169, 27396,     1,  1191,  5432,  1080, 20055,\n",
       "          57,   562,     1, 22671,    40,    35,   169,  2567,     0,\n",
       "       42665,  7761,   110,     0,     0, 41281,     0,   110,     0,\n",
       "       35891,   110,     0, 28781,    57,   169,  1419,     1, 11670,\n",
       "           0, 19470,     1,     0,     0,   169, 35071,    40,   562,\n",
       "          35, 12398,   657,  4857])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可见padding之后前面的tokens全变为0， 文本在最后面\n",
    "train_pad[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备target 向量， 前2000样本为正面评价，我们定义为1， 后2000为0\n",
    "train_target = np.concatenate((np.ones(len(pos_txts)), np.zeros(len(neg_txts))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入split来进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% 样本用于训练，剩余10%用于测试， 注意random_state = 12帮助筛选测试集时打乱顺序\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配备有彩电台灯圆桌上配备有 水壶和4个杯子 房内装有可自己调节的空调最满意的一是房间内基本没有许多宾馆常闻到的 气味是好的； 枕头被子床单凳床上用品很整洁干净明显应该是新添置的用品卫生间也较干净全天有热水供应就是隔音效果略微差点隔壁房间客人开关卫生间的水龙头有明显的水响声传过来每天服务员及时清理房间举止规范文明总体评价房间作为个人休息是比较满意的宾馆一楼 旁边设有餐厅每天早餐为自助餐收费10元可打入房费里一般有油条炸面饼包子馒头 煮鸡蛋茶叶蛋煎  粥 豆浆牛奶 咸菜 等 供应还是蛮丰富的比较 从宾馆出门可以很方便地打到出租车宾馆东边不远处是著名的 西边不远处是 打的从  向东去西单约12元车费我有天晚上从西单 回宾馆花了约1小时大家若没事可以去四处转转瞧瞧方便宾馆周围有些小吃点 等饮食 吃东西十分方便出宾馆后 200  24小时营业的便利店可以随时买到很多适合消费的生活用品\n",
      "class:  1.0\n"
     ]
    }
   ],
   "source": [
    "# 查看训练样本，确认无误\n",
    "print(reverse_tokens(X_train[35]))\n",
    "print('class: ',y_train[35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们用keras搭建LSTM模型，模型的第一层是Embedding层，只有当我们把tokens索引转换为词向量矩阵之后，才可以用神经网络对文本进行处理。 keras提供了Embedding接口，避免了繁琐的稀疏矩阵操作。\n",
    "在Embedding层我们输入的矩阵为：$$(batchsize, maxtokens)$$ 输出矩阵为： $$(batchsize, maxtokens, embeddingdim)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用LSTM对样本进行分类,sequential类型的model，不断添加层\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CaiTi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# 模型第一层为embedding\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Bidirectional(LSTM(units=32, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=red size=3> ***GRU：*** </font>\n",
    "如果使用GRU的话，测试样本可以达到87%的准确率，但GRU最后一层激活函数的输出都在0.5左右，说明模型的判断不是很明确，信心比较低，而且经过测试发现模型对于否定句的判断有时会失误，我们期望对于负面样本输出接近0，正面样本接近1而不是都徘徊于0.5之间。\n",
    "\n",
    "<font color=red size=3> ***BiLSTM：*** </font>\n",
    "测试了LSTM和BiLSTM，发现BiLSTM的表现最好，LSTM的表现略好于GRU，这可能是因为BiLSTM对于比较长的句子结构有更好的记忆\n",
    "Embedding之后第，一层我们用BiLSTM返回sequences，然后第二层16个单元的LSTM不返回sequences，只返回最终结果，最后是一个全链接层，用sigmoid激活函数输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU的代码\n",
    "# model.add(GRU(units=32, return_sequences=True))\n",
    "# model.add(GRU(units=16, return_sequences=True))\n",
    "# model.add(GRU(units=4, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加全连接层\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 这里使用adam优化器以0.001的learning rate进行优化\n",
    "optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function 使用交叉熵\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 238, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 238, 64)           85248     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                5184      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,090,449\n",
      "Trainable params: 90,449\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 检查一下模型的结构，一共90k左右可训练的变量\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "path_checkpoint = 'sentiment_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试加载已训练模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-5, patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3120 samples, validate on 347 samples\n",
      "WARNING:tensorflow:From C:\\Users\\CaiTi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "3072/3120 [============================>.] - ETA: 0s - loss: 0.6416 - acc: 0.6208\n",
      "Epoch 00001: val_loss improved from inf to 0.56939, saving model to sentiment_checkpoint.keras\n",
      "3120/3120 [==============================] - 20s 6ms/sample - loss: 0.6403 - acc: 0.6228 - val_loss: 0.5694 - val_acc: 0.7061\n",
      "Epoch 2/20\n",
      "3072/3120 [============================>.] - ETA: 0s - loss: 0.4808 - acc: 0.7839\n",
      "Epoch 00002: val_loss improved from 0.56939 to 0.47980, saving model to sentiment_checkpoint.keras\n",
      "3120/3120 [==============================] - 18s 6ms/sample - loss: 0.4805 - acc: 0.7833 - val_loss: 0.4798 - val_acc: 0.7896\n",
      "Epoch 3/20\n",
      "3072/3120 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8496\n",
      "Epoch 00003: val_loss did not improve from 0.47980\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "3120/3120 [==============================] - 18s 6ms/sample - loss: 0.3788 - acc: 0.8494 - val_loss: 0.4808 - val_acc: 0.7752\n",
      "Epoch 4/20\n",
      "3072/3120 [============================>.] - ETA: 0s - loss: 0.3456 - acc: 0.8630\n",
      "Epoch 00004: val_loss improved from 0.47980 to 0.40405, saving model to sentiment_checkpoint.keras\n",
      "3120/3120 [==============================] - 19s 6ms/sample - loss: 0.3446 - acc: 0.8638 - val_loss: 0.4040 - val_acc: 0.8444\n",
      "Epoch 5/20\n",
      "3072/3120 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.8747\n",
      "Epoch 00005: val_loss did not improve from 0.40405\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "3120/3120 [==============================] - 18s 6ms/sample - loss: 0.3232 - acc: 0.8753 - val_loss: 0.4079 - val_acc: 0.8329\n",
      "Epoch 6/20\n",
      "3072/3120 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.8779\n",
      "Epoch 00006: val_loss did not improve from 0.40405\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "3120/3120 [==============================] - 18s 6ms/sample - loss: 0.3156 - acc: 0.8776 - val_loss: 0.4044 - val_acc: 0.8386\n",
      "Epoch 7/20\n",
      "3072/3120 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.8809\n",
      "Epoch 00007: val_loss improved from 0.40405 to 0.40361, saving model to sentiment_checkpoint.keras\n",
      "3120/3120 [==============================] - 18s 6ms/sample - loss: 0.3132 - acc: 0.8808 - val_loss: 0.4036 - val_acc: 0.8386\n",
      "Epoch 8/20\n",
      "3072/3120 [============================>.] - ETA: 0s - loss: 0.3115 - acc: 0.8805\n",
      "Epoch 00008: val_loss did not improve from 0.40361\n",
      "3120/3120 [==============================] - 19s 6ms/sample - loss: 0.3117 - acc: 0.8808 - val_loss: 0.4039 - val_acc: 0.8386\n",
      "Epoch 9/20\n",
      "3072/3120 [============================>.] - ETA: 0s - loss: 0.3111 - acc: 0.8825\n",
      "Epoch 00009: val_loss did not improve from 0.40361\n",
      "3120/3120 [==============================] - 19s 6ms/sample - loss: 0.3105 - acc: 0.8827 - val_loss: 0.4038 - val_acc: 0.8357\n",
      "Epoch 10/20\n",
      "3072/3120 [============================>.] - ETA: 0s - loss: 0.3100 - acc: 0.8815\n",
      "Epoch 00010: val_loss did not improve from 0.40361\n",
      "3120/3120 [==============================] - 19s 6ms/sample - loss: 0.3094 - acc: 0.8811 - val_loss: 0.4045 - val_acc: 0.8300\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ee208169e8>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "model.fit(X_train, y_train,\n",
    "          validation_split=0.1, \n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们首先对测试样本进行预测，得到了还算满意的准确度。\n",
    "之后我们定义一个**预测函数**，来预测输入的文本的极性，可见模型对于否定句和一些简单的逻辑结构都可以进行准确的判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 1s 3ms/sample - loss: 0.3299 - acc: 0.8756\n",
      "Accuracy:87.56%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('是一例正面评价','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('是一例负面评价','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "酒店设施不是新的，服务态度很不好\n",
      "是一例负面评价 output=0.21\n",
      "酒店卫生条件非常不好\n",
      "是一例负面评价 output=0.17\n",
      "床铺非常舒适\n",
      "是一例正面评价 output=0.76\n",
      "房间很凉爽，空调冷气很足\n",
      "是一例负面评价 output=0.45\n",
      "酒店环境不好，住宿体验很不好\n",
      "是一例负面评价 output=0.13\n",
      "房间隔音不到位\n",
      "是一例负面评价 output=0.22\n",
      "晚上回来发现没有打扫卫生\n",
      "是一例负面评价 output=0.29\n",
      "因为过节所以要我临时加钱，比团购的价格贵\n",
      "是一例负面评价 output=0.10\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '酒店设施不是新的，服务态度很不好',\n",
    "    '酒店卫生条件非常不好',\n",
    "    '床铺非常舒适',\n",
    "    '房间很凉爽，空调冷气很足',\n",
    "    '酒店环境不好，住宿体验很不好',\n",
    "    '房间隔音不到位' ,\n",
    "    '晚上回来发现没有打扫卫生',\n",
    "    '因为过节所以要我临时加钱，比团购的价格贵'\n",
    "]\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 寻找错误分类的文本 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.T[0]\n",
    "y_pred = [1 if p>= 0.5 else 0 for p in y_pred]\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出错误分类的索引\n",
    "misclassified = np.where( y_pred != y_actual )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386\n"
     ]
    }
   ],
   "source": [
    "# 输出所有错误分类的索引\n",
    "len(misclassified)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                              客户反映 酒店的服务和房间也就是2星级标准尤其服务及 都不尽人意令人失望但在网上 在推荐第一位置请你们要按照实际情况来向客人推荐是一家很差的 反馈2008年5月30日：我酒店一直非常重视 客人给我们的评价对于客人提出的意见我们会及时的进行改正和完善今天我们发现点评  客人对我们的评价很差于是及时联系了实际住宿客人  反映在酒店住宿期间对客房服务都很满意点评是其朋友填写的 时尚 真诚欢迎各界朋友来我酒店入住也希望 的朋友有机会来亲身体验我们将努力为您提供热情优质的服务\n",
      "预测的分类 1\n",
      "实际的分类 0.0\n"
     ]
    }
   ],
   "source": [
    "# 我们来找出错误分类的样本看看\n",
    "idx=101\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                          5月底入住的房间装修感觉还不错就是房间小了 居然还是 拼起来的有点奇怪地理位置不错正对着 离 地铁站也只需要过 马路离 上的酒吧也比较近特别是去九龙的话下楼就是 轮的码头很方便酒店每天会送一个小玩具虽然很简单但是 的攒在一起可以摆出很多造型一直玩到现在不知道下次去的时候 不会送酒店的餐厅比较一般价格也贵还不如去地铁站 或者 达道上的  来的实惠游泳池很好可惜开的时间短了点要是晚上也开 场的话就更好了大堂的结账比较怪异结账的时候取消信用卡 居然不撕小票有点不习惯不知道是不是这个酒店的一贯做法还是香港的惯例\n",
      "预测的分类 1\n",
      "实际的分类 1.0\n"
     ]
    }
   ],
   "source": [
    "idx=1\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这里的问题是在训练样本中把店家的反馈也放进去了。。。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
